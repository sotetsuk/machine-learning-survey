# Attention

# Basics
- Sutskever et al., "Sequence to sequence learning with neural networks." NIPS, 2014. [[pdf](http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf)]
- Bahdanau et al., "Neural machine translation by jointly learning to align and translate." ICLR, 2015. [[pdf](https://arxiv.org/pdf/1409.0473.pdf)]

# Application

## Natural Lnaguage Processing

### Neural Machine Translation

### Neural Language Correction
- Xie et al., "Neural Language Correction with Character-Based Attention." arXiv:1603.09727, 2006. [[pdf](https://arxiv.org/pdf/1603.09727.pdf)]

### Grammer
- Kuncoro et al., "What Do Recurrent Neural Network Grammars Learn About Syntax." arXiv:1611.05774, 2016. [[pdf](https://arxiv.org/pdf/1611.05774v1.pdf)]

## Image Processing
### Caption
Xu et al., "Show, attend and tell: Neural image caption generation with visual attention." ICLR2015. [[pdf](http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf)]

## Reinforcement Learning
- Sorokin et al., "Deep Attention Recurrent Q-Network." NIPS Workshop, 2014. [[pdf](https://arxiv.org/pdf/1512.01693v1.pdf)]

# Others
- Bahdanau et al., "An Actor-Critic Algorithm for Sequence Prediction." arXiv:1607.07086, 2016. [[pdf](https://arxiv.org/pdf/1607.07086v2.pdf)]
- Tu et al., "Neural Machine Translation with Reconstruction." arXiv:1611.01874, 2016. [[pdf](https://arxiv.org/pdf/1611.01874v1.pdf)]
- Kalchbrenner et al., "Neural Machine Translation in Linear Time." arXiv:1610.10099, 2016. [[pdf](https://arxiv.org/pdf/1610.10099v1.pdf)] 
- Kočiský et al., "Semantic Parsing with Semi-Supervised Sequential Autoencoders." arXiv:1609.09315, 2016. [[pdf](https://arxiv.org/pdf/1609.09315v1.pdf)]
- Gu et al., "Incorporating Copying Mechanism in Sequence-to-Sequence Learning." ACL, 2016, [[pdf](http://aclweb.org/anthology/P/P16/P16-1154.pdf)][[関連資料](http://www.lr.pi.titech.ac.jp/~sasano/acl2016suzukake/slides/08.pdf)]
